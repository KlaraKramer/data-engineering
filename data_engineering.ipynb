{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "VUMz0TfYreoF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "noIP3LAQ-Gs7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "WmEolL1nsm00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing value handling\n",
        "\n",
        "The following three functions are for use in the 'Missing value handling' stage.\n",
        "\n",
        "**detect_missing_values(df)**\n",
        "- Input: raw dataframe df\n",
        "- Output: (1) summary dataframe that contains the number of missing values per column of df , and (2) the total number of missing values within df\n",
        "\n",
        "**impute_missing_values(df, method)**\n",
        "- Input: (1) raw dataframe df, and (2) [optionally] the method of imputation ('simple' / 'KNN')\n",
        "- Output: dataframe df with all missing values imputed by either ('simple') replacing them with the column average, or ('KNN') inferring them using the k nearest neighbours\n",
        "\n",
        "**remove_missing_values(df)**\n",
        "- Input: raw dataframe df\n",
        "- Output: dataframe df with all rows containing missing values removed"
      ],
      "metadata": {
        "id": "9Fqp35fgZ6OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_missing_values(df):\n",
        "    # Count the number of missing values detected in each column\n",
        "    missing_df = df.isnull().sum()\n",
        "    missing_df = pd.DataFrame(missing_df)\n",
        "    missing_df = missing_df.T\n",
        "    # Count the total number of missing values\n",
        "    missing_count = int(missing_df.sum().sum())\n",
        "    return missing_df, missing_count\n",
        "\n",
        "def impute_missing_values(df, method='simple'):\n",
        "    df_copy = df.copy()  # Avoid modifying the original DataFrame\n",
        "    # Select only numeric columns\n",
        "    num_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    if method == 'simple':\n",
        "        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    if method == 'KNN':\n",
        "        imp = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
        "    else:\n",
        "        # Default to the simple imputer\n",
        "        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "    # Apply imputation only to numeric columns\n",
        "    df_copy[num_cols] = imp.fit_transform(df_copy[num_cols])\n",
        "    return df_copy\n",
        "\n",
        "def remove_missing_values(df):\n",
        "    df = df[df.notnull().all(axis=1)]\n",
        "    return df"
      ],
      "metadata": {
        "id": "DPtXK39asp8A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicate removal\n",
        "\n",
        "The following two functions are for use in the 'Duplicate removal' stage.\n",
        "\n",
        "**detect_duplicates(df)**\n",
        "- Input: raw dataframe df\n",
        "- Output: (1) dataframe df with an additional column that contains 'True' if the row is a duplicate, and 'False' otherwise, and (2) the total number of duplicated rows within df\n",
        "\n",
        "**remove_duplicates(df)**\n",
        "- Input: raw dataframe df\n",
        "- Output: dataframe df with all duplicated rows removed"
      ],
      "metadata": {
        "id": "xAWG28EObbKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_duplicates(df):\n",
        "    # Reset previous detection steps\n",
        "    if 'duplicate' in df.columns:\n",
        "        df = df.drop('duplicate', axis=1)\n",
        "    df_copy = df\n",
        "    # Remove interfering columns\n",
        "    if 'id' in df_copy.columns:\n",
        "        df_copy = df_copy.drop('id', axis=1)\n",
        "    # Add a new column\n",
        "    df['duplicate'] = df_copy.duplicated(keep='first')\n",
        "    # Count the number of duplicates detected\n",
        "    dups_count = df['duplicate'].value_counts().get(True, 0)\n",
        "    return df, dups_count\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    duplicate_df, dups_count = detect_duplicates(df)\n",
        "    duplicate_df = duplicate_df[duplicate_df.duplicate != True]\n",
        "    return duplicate_df"
      ],
      "metadata": {
        "id": "2Q5NE4FsbbYb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outlier handling\n",
        "\n",
        "The following two functions are for use in the 'Outlier handling' stage.\n",
        "\n",
        "**detect_outliers(df, contamination)**\n",
        "- Input: (1) raw dataframe df, and (2) a percentage between 0 and 0.5 that determines how many outliers are to be detected\n",
        "- Output: (1) dataframe df with an additional column that contains 'True' if the row contains an outlier, and 'False' otherwise, and (2) the total number of outliers within df\n",
        "\n",
        "**remove_outliers(df, contamination)**\n",
        "- Input: raw dataframe df, and (2) a percentage between 0 and 0.5 that determines how many outliers are to be detected\n",
        "- Output: dataframe df with all outliers removed"
      ],
      "metadata": {
        "id": "9zaxRXXacM4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers(data, contamination):\n",
        "    # Make a copy of the data to retain original categorical labels\n",
        "    data_original = data.copy()\n",
        "\n",
        "    # Convert to Pandas DataFrame if necessary\n",
        "    if not isinstance(data_original, pd.DataFrame):\n",
        "        data_original = pd.DataFrame(data_original)\n",
        "\n",
        "    # Convert object columns to numerical for model training\n",
        "    data_converted = data_original.copy()\n",
        "    object_cols = data_converted.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Check if column is datetime column or categorical column\n",
        "    for col in object_cols:\n",
        "        try:\n",
        "            parsed_col = pd.to_datetime(col, errors='coerce', infer_datetime_format=True)\n",
        "            timestamp_ratio = parsed_col.notna().mean()  # Proportion of successfully converted values\n",
        "            if timestamp_ratio > 0.9:  # If most values convert successfully, treat it as a timestamp\n",
        "                # Convert timestamp column to integer\n",
        "                year = [int(year_str[:4]) for year_str in data_converted[col]]\n",
        "                data_converted[col] = year\n",
        "        except Exception:\n",
        "            # Treat it as a categorical column\n",
        "            le = LabelEncoder()\n",
        "            data_converted[col] = le.fit_transform(data_converted[col])  # Encode categories numerically\n",
        "\n",
        "    # Train the detection model\n",
        "    iso = IsolationForest(contamination=contamination)\n",
        "    yhat = iso.fit_predict(data_converted)\n",
        "\n",
        "    # Apply predictions to the original DataFrame\n",
        "    data_original['outlier'] = yhat == -1\n",
        "\n",
        "    # Count outliers\n",
        "    outlier_count = data_original['outlier'].sum()\n",
        "\n",
        "    return data_original, outlier_count\n",
        "\n",
        "def remove_outliers(df, contamination):\n",
        "    outlier_df, outlier_count = detect_outliers(df, contamination)\n",
        "    outlier_df = outlier_df[outlier_df.outlier != True]\n",
        "    return outlier_df"
      ],
      "metadata": {
        "id": "YCR3jNYwcNAc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading\n",
        "\n",
        "**downloadable_data(df)** is a helper function that will be used when downloading the final cleaned data. You will not need to call this function."
      ],
      "metadata": {
        "id": "kehHMqz9c2Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downloadable_data(df):\n",
        "    if 'duplicate' in df.columns:\n",
        "        df = df.drop(columns=['duplicate'])\n",
        "    if 'outlier' in df.columns:\n",
        "        df = df.drop(columns=['outlier'])\n",
        "    return df"
      ],
      "metadata": {
        "id": "FJoRzMH-c1iT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re4Ckdyj_ZZt"
      },
      "source": [
        "# Data loading\n",
        "\n",
        "**Please do not modify the below code**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_car = 'https://github.com/KlaraKramer/data-engineering-datasets/raw/main/corrupted-datasets/corrupted_car.csv'\n",
        "url_energy = 'https://github.com/KlaraKramer/data-engineering-datasets/raw/main/corrupted-datasets/corrupted_energy.csv'\n",
        "\n",
        "car_insurance_df = pd.read_csv(url_car)\n",
        "energy_df = pd.read_csv(url_energy)\n",
        "\n",
        "# Convert all column names to lowercase and replace spaces with underscores\n",
        "car_insurance_df.rename(columns=lambda x: x.lower().replace(' ', '_').replace('-', '_'), inplace=True)\n",
        "energy_df.rename(columns=lambda x: x.lower().replace(' ', '_').replace('-', '_'), inplace=True)\n",
        "# Remove all special characters from column names\n",
        "car_insurance_df.rename(columns=lambda x: x.lower().replace(':', '').replace('$', '').replace('(', '').replace(')', ''), inplace=True)\n",
        "energy_df.rename(columns=lambda x: x.lower().replace(':', '').replace('$', '').replace('(', '').replace(')', ''), inplace=True)"
      ],
      "metadata": {
        "id": "XPFbRviEWYi2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is where your data cleaning work begins\n",
        "\n",
        "**Please fill out the below three code cells using the functions provided to you above**"
      ],
      "metadata": {
        "id": "oazPEXtsdnXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing value handling\n",
        "\n",
        "\n",
        "**-> Modify car_insurance_df and energy_df to not contain missing values.**\n",
        "\n",
        "\n",
        "Functions you can use are:\n",
        "\n",
        "**detect_missing_values(df)**\n",
        "\n",
        "**impute_missing_values(df, method)**\n",
        "\n",
        "**remove_missing_values(df)**\n",
        "\n",
        "Feel free to scroll back up to remind yourself of what these functions do."
      ],
      "metadata": {
        "id": "QG11S7HCrsi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choice of:\n",
        "# 1: Removing rows with missing values\n",
        "# or\n",
        "# 2: Imputing missing values (Replacing them with the average value)\n",
        "# or\n",
        "# 3: Imputing missing values (Replacing them with neighbouring values)"
      ],
      "metadata": {
        "id": "9bzfrFwHsMBk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VedEC__SBD1-"
      },
      "source": [
        "## Duplicate removal\n",
        "\n",
        "**-> Modify car_insurance_df and energy_df to not contain duplicated rows.**\n",
        "\n",
        "Functions you can use are:\n",
        "\n",
        "**detect_duplicates(df)**\n",
        "\n",
        "**remove_duplicates(df)**\n",
        "\n",
        "Feel free to scroll back up to remind yourself of what these functions do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dtJW9-qRDI9P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM8md_m6BHiJ"
      },
      "source": [
        "## Outlier handling\n",
        "\n",
        "**-> Modify car_insurance_df and energy_df to not contain outliers.**\n",
        "\n",
        "Functions you can use are:\n",
        "\n",
        "**detect_outliers(df, contamination)**\n",
        "\n",
        "**remove_outliers(df, contamination)**\n",
        "\n",
        "Feel free to scroll back up to remind yourself of what these functions do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rNRzJviMRn9W"
      },
      "outputs": [],
      "source": [
        "# Choice of how many outiers to detect (contamination percentage)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading\n",
        "\n",
        "**Please do not modify the below code**"
      ],
      "metadata": {
        "id": "9yAOVBOPsCb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_insurance_df = downloadable_data(car_insurance_df)\n",
        "car_insurance_df.to_csv('car_clean.csv', index=False)\n",
        "files.download('car_clean.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "olwsOLGksB33",
        "outputId": "2f16e464-697f-4d76-d735-a1be5abc8f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_27990570-bfb2-455c-8f34-27f9f5d335ba\", \"car_clean.csv\", 47245)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "energy_df = downloadable_data(energy_df)\n",
        "energy_df.to_csv('energy_clean.csv', index=False)\n",
        "files.download('energy_clean.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WdRSUh-We4-z",
        "outputId": "f1b4c2cb-5752-4915-dfdd-6f907b178ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1a24c341-f009-49fd-81f9-4f18ebdd5118\", \"energy_clean.csv\", 5666)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please now also download the .ipynb file as shown in the video:**\n",
        "\n",
        "\n",
        "1.   On the top left of your screen, click 'File'\n",
        "2.   Select 'Download'\n",
        "3.   Select 'Download .ipynb'\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HuO9Uz48dNAH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}